% K-means Clustering
% Jon Craton

For this project, I have been asked to document a Jupyer notebook, and then make some parameter adjustments to explore subsequent behavior. Let's first explore the Jupyter notebook. I've opted to do this without using Jupyter directly, as I simply prefer other tools.

Code Extraction
===============

We can extract the source code from the supplied notebook quite easily:

```python
import json

with open('pythonKMC.ipynb') as nb:
  src = [''.join(c['source']) for c in json.load(nb)['cells']]
  [print(line) for line in src[:4]]
```
…

Documentation
=============

Let's simply include the full source here and add inline documentation to the original file.

Imports
-------

We'll start out with a set of imports suggesting that we'll be performing a K-means clustering experiment using generated datasets.

```python
from sklearn.datasets.samples_generator import make_blobs
import numpy as np
np.random.seed(6)
import math
from collections import Counter
from sklearn import datasets
#Default plot params
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
```

Data Generation
---------------

Now let's generate some test data. It consists of 5000 points clustered around 3 different means, with a standard deviation of 3 about each center.

### Type 1

```python
centers_type_one = [(-10, 10), (0, -5), (10, 5)]
x_type_one, Y_type_one_labels = datasets.make_blobs(n_samples=5000,
                                centers=centers_type_one,
                                cluster_std=3,
                                random_state=2)
```

Let's plot that:

```python,results='hidden'
plt.scatter(x_type_one[:,0],x_type_one[:,1],marker='o',c =
Y_type_one_labels )
```

### Type 2

Let's generate a second dataset. It consists of 5000 points from 2 classes. Each class consists of two clusters, so we will have 4 clusters total, with 2 from each class.

```python
x_type_two, Y_type_two_labels = datasets.make_classification(n_samples=5000,
                                          n_features=2, n_redundant=0,
                                          n_informative=2,
                                          n_classes=2,
                                          n_clusters_per_class=2,
                                          class_sep=3,
                                          shuffle=True,
                                          random_state= 101)
```

And plot that:

```python,results='hidden'
plt.scatter(x_type_two[:,0],x_type_two[:,1],marker='o',c =
Y_type_two_labels )
```

K-Means Training
----------------

We now train a K-means clustering model.

K-means clusting is an unsupervised learning model. It seeks to partition our events into k groups. These groups are defined by the center of mass of the events. Ideally, this allows us to reduce a large sample into just a few points that are representative of a few groups present in our data set.

Once we have trained our model and identified our clusters, we can then use this as a classifier to determine group membership of new observations.

Let's take a look at how this appears in practice.

### Using all training data (n=5000)

#### Type 1

```python,results='hidden'
km_type_one = KMeans(n_clusters=3,
random_state=2).fit_predict(x_type_one)
plt.scatter(x_type_one[:,0], x_type_one[:,1], c=km_type_one)
```

#### Type 2

```python,results='hidden'
km_type_two = KMeans(n_clusters=2,
random_state=2).fit_predict(x_type_two)
plt.scatter(x_type_two[:,0], x_type_two[:,1], c=km_type_two)
```

### Using reduced training data (n=500)

```python
M = 500  
```

#### Type 1

```python,results='hidden'
km_type_one = KMeans(n_clusters=3,
random_state=2).fit_predict(x_type_one[-M:])
plt.scatter(x_type_one[-M:,0], x_type_one[-M:,1], c=km_type_one)
```

#### Type 2

```python,results='hidden'
km_type_two = KMeans(n_clusters=2,
random_state=2).fit_predict(x_type_two[-M:])
plt.scatter(x_type_two[-M:,0], x_type_two[-M:,1], c=km_type_two)
```

The overall purpose of this code was to generate two different datasets, train a K-means model to identify clusters, and then plot the membership of values in these clusters.

Impact of Cluster δ
===================

Let's explore the impact of adjusting the cluster σ.

```python
from sklearn.metrics import accuracy_score,hamming_loss

for σ in [.5,1,1.5,2,2.5,3]:
  centers_type_one = [(-10, 10), (0, -5), (10, 5)]
  x, Y = datasets.make_blobs(n_samples=5000,
                             centers=centers_type_one,
                             cluster_std=σ,
                             random_state=2)
  
  km = KMeans(n_clusters=3,
  random_state=2).fit_predict(x)
  plt.scatter(x[:,0], x[:,1], c=km)

  plt.title("σ=%.01f" % (σ))
  plt.show()
```

We can see that as we increase σ, our clusters begin to overlap, and we start to see mispredictions.

Impact of Class Separation
==========================

```python
from sklearn.metrics import accuracy_score,hamming_loss

for sep in [.5,1,1.5,2,2.5,3]:
  centers_type_one = [(-10, 10), (0, -5), (10, 5)]
  x, Y = datasets.make_classification(n_samples=5000,
                                      n_features=2, n_redundant=0,
                                      n_informative=2,
                                      n_classes=2,
                                      n_clusters_per_class=2,
                                      class_sep=sep,
                                      shuffle=True,
                                      random_state= 101)  
  km = KMeans(n_clusters=2,
  random_state=2).fit_predict(x)
  plt.scatter(x[:,0], x[:,1], c=km)

  plt.title("class_sep=%.01f" % (sep))
  plt.show()
```

We see that the impact of class separation is the opposite of σ. When the class separation is too small, the model is unable to distinguish between the groups.

Clusters Per Class
==================

```python
from sklearn.metrics import accuracy_score,hamming_loss

for cpc in [1,2]:
  centers_type_one = [(-10, 10), (0, -5), (10, 5)]
  x, Y = datasets.make_classification(n_samples=5000,
                                      n_features=2, n_redundant=0,
                                      n_informative=2,
                                      n_classes=2,
                                      n_clusters_per_class=cpc,
                                      class_sep=2,
                                      shuffle=True,
                                      random_state= 101)  
  km = KMeans(n_clusters=2,
  random_state=2).fit_predict(x)

  plt.scatter(x[:,0],x[:,1],marker='o',c = Y )
  plt.title("Dataset, clusters_per_class=%.01f" % (cpc))
  plt.show()
  
  plt.scatter(x[:,0], x[:,1], c=km)
  plt.title("K-means clusters, clusters_per_class=%.01f" % (cpc))
  plt.show()
```

As the name implies, the number of clusters per class impact how many distinct groupings we see for each class in our plot.

