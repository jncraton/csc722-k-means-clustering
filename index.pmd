% K-means Clustering
% Jon Craton

For this project, I have been asked to document a Jupyer notebook, and then make some parameter adjustments to explore subsequent behavior. Let's first explore the Jupyter notebook. I've opted to do this without using Jupyter directly, as I simply prefer other tools.

Code Extraction
===============

We can extract the source code from the supplied notebook quite easily:

```python
import json

with open('pythonKMC.ipynb') as nb:
  content = json.load(nb)
  src = [''.join(c['source']) for c in content['cells']]

```

Documentation
=============

Let's simply include the full source here and add inline documentation.

Imports
-------

We'll start out with a set of imports suggesting that we'll be performing a K-means clustering experiment using generated datasets.

```python
from sklearn.datasets.samples_generator import make_blobs
import numpy as np
np.random.seed(6)
import math
from collections import Counter
from sklearn import datasets
#Default plot params
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
```

Data Generation
---------------

Now let's generate some test data. It consists of 5000 points clustered around 3 different means, with a standard deviation of 3 about each center.

```python
centers_type_one = [(-10, 10), (0, -5), (10, 5)]
x_type_one, Y_type_one_labels = datasets.make_blobs(n_samples=5000,
                                centers=centers_type_one,
                                cluster_std=3,
                                random_state=2)
```

Let's generate a second dataset. It consists of 5000 points from 2 classes. Each point 

```python
x_type_two, Y_type_two_labels = datasets.make_classification(n_samples=5000,
                                          n_features=2, n_redundant=0,
                                          n_informative=2,
                                          n_classes=2,
                                          n_clusters_per_class=2,
                                          class_sep=3,
                                          shuffle=True,
                                          random_state= 101)
```

Data Plots
----------

Now let's plot our first datasets:

```python
plt.figure(figsize=(15,8))
plt.subplot(121, title='"type_one" K-Means')
plt.scatter(x_type_one[:,0],x_type_one[:,1],marker='o',c =
Y_type_one_labels )
plt.subplot(122, title='"type_two" K-Means')
plt.scatter(x_type_two[:,0],x_type_two[:,1],marker='o',c =
Y_type_two_labels )
plt.show()
```

K-Means Training
----------------

We now train a K-means clustering model.

### Using all training data (n=5000)

```python
#Predict K-Means cluster membership
km_type_one = KMeans(n_clusters=3,
random_state=2).fit_predict(x_type_one)
km_type_two = KMeans(n_clusters=2,
random_state=2).fit_predict(x_type_two)

plt.figure(figsize=(15,8))
plt.subplot(121, title='"type_one" K-Means')
plt.scatter(x_type_one[:,0], x_type_one[:,1], c=km_type_one)
plt.subplot(122, title='"type_two" K-Means')
plt.scatter(x_type_two[:,0], x_type_two[:,1], c=km_type_two)
```

### Using reduced training data (n=500)

```python
# I have chosed M last samples (if you look at cell 8 and cell 9 in this notebook)
M = 500  

#Predict K-Means cluster membership
km_type_one = KMeans(n_clusters=3,
random_state=2).fit_predict(x_type_one[-M:])
km_type_two = KMeans(n_clusters=2,
random_state=2).fit_predict(x_type_two[-M:])

plt.figure(figsize=(15,8))
plt.subplot(121, title='"type_one" K-Means')
plt.scatter(x_type_one[-M:,0], x_type_one[-M:,1], c=km_type_one)
plt.subplot(122, title='"type_two" K-Means')
plt.scatter(x_type_two[-M:,0], x_type_two[-M:,1], c=km_type_two)
```
